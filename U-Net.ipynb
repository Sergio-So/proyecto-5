{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DoubleConvolution(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.first = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.second = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.first(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.second(x)\n",
    "        x = self.act2(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.pool(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.up(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class CropAndConcat(nn.Module):\n",
    "    def forward(self, x: torch.Tensor, contracting_x: torch.Tensor):\n",
    "        contracting_x = torchvision.transforms.functional.center_crop(contracting_x, [x.shape[2], x.shape[3]])\n",
    "        x = torch.cat([x, contracting_x], dim=1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.down_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(in_channels, 64), (64, 128), (128, 256), (256, 512)]])\n",
    "        self.down_sample = nn.ModuleList([DownSample() for _ in range(4)])\n",
    "        self.middle_conv = DoubleConvolution(512, 1024)\n",
    "        self.up_sample = nn.ModuleList([UpSample(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "        self.up_conv = nn.ModuleList([DoubleConvolution(i, o) for i, o in [(1024, 512), (512, 256), (256, 128), (128, 64)]])\n",
    "        self.concat = nn.ModuleList([CropAndConcat() for _ in range(4)])\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pass_through = []\n",
    "        for i in range(len(self.down_conv)):\n",
    "            x = self.down_conv[i](x)\n",
    "            pass_through.append(x)\n",
    "            x = self.down_sample[i](x)\n",
    "        x = self.middle_conv(x)\n",
    "        for i in range(len(self.up_conv)):\n",
    "            x = self.up_sample[i](x)\n",
    "            x = self.concat[i](x, pass_through.pop())\n",
    "            x = self.up_conv[i](x)\n",
    "        x = self.final_conv(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import nibabel as nib\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class PancreasCTDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir_path, target_dir_path, transform=None, target_transform=None):\n",
    "        self.sample = self.make_dataset(annotation_file, img_dir_path, target_dir_path)\n",
    "        self.img_dir_path = img_dir_path\n",
    "        self.target_dir_path = target_dir_path\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataset(annotation_file, img_dir_path, target_dir_path):\n",
    "        dataset = []\n",
    "        pancreas_location = pd.read_csv(annotation_file)[['Subject ID', 'File Location']].to_numpy()\n",
    "        for pancreas_name, pancreas_path in pancreas_location:\n",
    "            target_filename = 'label' + pancreas_name[-4:] + '.nii.gz'\n",
    "            target_path = os.path.join(target_dir_path, target_filename)\n",
    "            pancreas_path = os.path.join(img_dir_path, pancreas_path)\n",
    "            pancreas_slice_filename = os.listdir(pancreas_path)\n",
    "            pancreas_slice_filename.sort()\n",
    "            for layer, filename in enumerate(pancreas_slice_filename):\n",
    "                pancreas_slice_path = os.path.join(pancreas_path, filename)\n",
    "                dataset.append((pancreas_slice_path, (target_path, layer)))\n",
    "        return dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pancreas_slice_path, target_slice_path = self.sample[idx]\n",
    "        slice = dicom.dcmread(pancreas_slice_path).pixel_array\n",
    "        target = np.moveaxis(nib.load(target_slice_path[0]).get_fdata(), -1, 0)[target_slice_path[1]]\n",
    "        if self.transform:\n",
    "            slice = self.transform(slice)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return slice, target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(in_channels=1, out_channels=1).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "annotation_file = Path('/home/ceduardosq/Code/Extracurricular/Pancreas/Data/Pancreas-CT20200910/metadata.csv')\n",
    "img_dir_path = Path('/home/ceduardosq/Code/Extracurricular/Pancreas/Data/Pancreas-CT20200910/')\n",
    "target_dir_path = Path('/home/ceduardosq/Code/Extracurricular/Pancreas/Data/TCIA_pancreas_labels-02-05-2017')\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])\n",
    "\n",
    "dataset = PancreasCTDataset(annotation_file=annotation_file, img_dir_path=img_dir_path, target_dir_path=target_dir_path)\n",
    "\n",
    "trainset, valset, testset = torch.utils.data.random_split(dataset=dataset, lengths=[0.7, 0.2, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "18695"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# for epoch in range(3):\n",
    "#     for i, batch in enumerate(trainloader, 0):\n",
    "#         slice, segment = batch\n",
    "#\n",
    "#         output = model(slice)\n",
    "#         loss = criterion(output, segment)\n",
    "#         loss.backwards()\n",
    "#         optimizer.step()\n",
    "#\n",
    "# print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
